torch version: 2.2.1
device: cuda
Adaptive step: 0
Epoch: 0 - Loss: 3.432e+01 - Learning Rate: 1.000e-03
Epoch: 2000 - Loss: 2.348e-01 - Learning Rate: 1.000e-03
Epoch: 4000 - Loss: 2.248e-01 - Learning Rate: 1.000e-03
Epoch: 6000 - Loss: 2.163e-01 - Learning Rate: 1.000e-03
Epoch: 8000 - Loss: 2.023e-01 - Learning Rate: 1.000e-03
Epoch: 10000 - Loss: 1.780e-01 - Learning Rate: 8.000e-04
Epoch: 12000 - Loss: 6.825e-02 - Learning Rate: 8.000e-04
Epoch: 14000 - Loss: 5.228e-02 - Learning Rate: 8.000e-04
Epoch: 16000 - Loss: 4.220e-02 - Learning Rate: 8.000e-04
Epoch: 18000 - Loss: 3.412e-02 - Learning Rate: 8.000e-04
Epoch: 20000 - Loss: 2.381e-02 - Learning Rate: 6.400e-04
Epoch: 22000 - Loss: 1.381e-02 - Learning Rate: 6.400e-04
Epoch: 24000 - Loss: 1.226e-02 - Learning Rate: 6.400e-04
Epoch: 26000 - Loss: 1.203e-02 - Learning Rate: 6.400e-04
Epoch: 28000 - Loss: 1.141e-02 - Learning Rate: 6.400e-04
Epoch: 30000 - Loss: 1.101e-02 - Learning Rate: 5.120e-04
Epoch: 32000 - Loss: 1.053e-02 - Learning Rate: 5.120e-04
Epoch: 34000 - Loss: 9.896e-03 - Learning Rate: 5.120e-04
Epoch: 36000 - Loss: 9.201e-03 - Learning Rate: 5.120e-04
Epoch: 38000 - Loss: 8.270e-03 - Learning Rate: 5.120e-04
Epoch: 40000 - Loss: 7.575e-03 - Learning Rate: 4.096e-04
Epoch: 42000 - Loss: 6.416e-03 - Learning Rate: 4.096e-04
Epoch: 44000 - Loss: 5.625e-03 - Learning Rate: 4.096e-04
Epoch: 46000 - Loss: 4.878e-03 - Learning Rate: 4.096e-04
Epoch: 48000 - Loss: 4.479e-03 - Learning Rate: 4.096e-04
Epoch: 50000 - Loss: 3.746e-03 - Learning Rate: 3.277e-04
Epoch: 52000 - Loss: 3.323e-03 - Learning Rate: 3.277e-04
Epoch: 54000 - Loss: 3.089e-03 - Learning Rate: 3.277e-04
Epoch: 56000 - Loss: 2.680e-03 - Learning Rate: 3.277e-04
Epoch: 58000 - Loss: 2.432e-03 - Learning Rate: 3.277e-04
Epoch: 60000 - Loss: 2.203e-03 - Learning Rate: 2.621e-04
Epoch: 62000 - Loss: 1.971e-03 - Learning Rate: 2.621e-04
Epoch: 64000 - Loss: 1.817e-03 - Learning Rate: 2.621e-04
Epoch: 66000 - Loss: 1.644e-03 - Learning Rate: 2.621e-04
Epoch: 68000 - Loss: 1.509e-03 - Learning Rate: 2.621e-04
Epoch: 70000 - Loss: 1.424e-03 - Learning Rate: 2.097e-04
Epoch: 72000 - Loss: 1.291e-03 - Learning Rate: 2.097e-04
Epoch: 74000 - Loss: 1.193e-03 - Learning Rate: 2.097e-04
Epoch: 76000 - Loss: 1.117e-03 - Learning Rate: 2.097e-04
Epoch: 78000 - Loss: 1.044e-03 - Learning Rate: 2.097e-04
Epoch: 80000 - Loss: 9.618e-04 - Learning Rate: 1.678e-04
Epoch: 82000 - Loss: 9.128e-04 - Learning Rate: 1.678e-04
Epoch: 84000 - Loss: 8.380e-04 - Learning Rate: 1.678e-04
Epoch: 86000 - Loss: 7.970e-04 - Learning Rate: 1.678e-04
Epoch: 88000 - Loss: 7.364e-04 - Learning Rate: 1.678e-04
Epoch: 90000 - Loss: 7.071e-04 - Learning Rate: 1.342e-04
Epoch: 92000 - Loss: 6.493e-04 - Learning Rate: 1.342e-04
Epoch: 94000 - Loss: 6.604e-04 - Learning Rate: 1.342e-04
Epoch: 96000 - Loss: 5.817e-04 - Learning Rate: 1.342e-04
Epoch: 98000 - Loss: 5.524e-04 - Learning Rate: 1.342e-04
Epoch: 100000 - Loss: 5.282e-04 - Learning Rate: 1.074e-04
Epoch: 102000 - Loss: 4.945e-04 - Learning Rate: 1.074e-04
Epoch: 104000 - Loss: 4.666e-04 - Learning Rate: 1.074e-04
Epoch: 106000 - Loss: 4.394e-04 - Learning Rate: 1.074e-04
Epoch: 108000 - Loss: 4.174e-04 - Learning Rate: 1.074e-04
Epoch: 110000 - Loss: 4.204e-04 - Learning Rate: 8.590e-05
Epoch: 112000 - Loss: 3.793e-04 - Learning Rate: 8.590e-05
Epoch: 114000 - Loss: 3.615e-04 - Learning Rate: 8.590e-05
Epoch: 116000 - Loss: 3.468e-04 - Learning Rate: 8.590e-05
Epoch: 118000 - Loss: 3.307e-04 - Learning Rate: 8.590e-05
Epoch: 120000 - Loss: 4.168e-04 - Learning Rate: 6.872e-05
Epoch: 122000 - Loss: 3.055e-04 - Learning Rate: 6.872e-05
Epoch: 124000 - Loss: 2.923e-04 - Learning Rate: 6.872e-05
Epoch: 126000 - Loss: 2.810e-04 - Learning Rate: 6.872e-05
Epoch: 128000 - Loss: 2.787e-04 - Learning Rate: 6.872e-05
Epoch: 130000 - Loss: 2.626e-04 - Learning Rate: 5.498e-05
Epoch: 132000 - Loss: 2.691e-04 - Learning Rate: 5.498e-05
Epoch: 134000 - Loss: 2.681e-04 - Learning Rate: 5.498e-05
Epoch: 136000 - Loss: 2.350e-04 - Learning Rate: 5.498e-05
Epoch: 138000 - Loss: 2.281e-04 - Learning Rate: 5.498e-05
Epoch: 140000 - Loss: 2.240e-04 - Learning Rate: 4.398e-05
Epoch: 142000 - Loss: 2.131e-04 - Learning Rate: 4.398e-05
Epoch: 144000 - Loss: 2.096e-04 - Learning Rate: 4.398e-05
Epoch: 146000 - Loss: 2.001e-04 - Learning Rate: 4.398e-05
Epoch: 148000 - Loss: 1.941e-04 - Learning Rate: 4.398e-05
Epoch: 150000 - Loss: 1.890e-04 - Learning Rate: 3.518e-05
Epoch: 152000 - Loss: 1.842e-04 - Learning Rate: 3.518e-05
Epoch: 154000 - Loss: 1.781e-04 - Learning Rate: 3.518e-05
Epoch: 156000 - Loss: 1.733e-04 - Learning Rate: 3.518e-05
Epoch: 158000 - Loss: 1.687e-04 - Learning Rate: 3.518e-05
Epoch: 160000 - Loss: 1.680e-04 - Learning Rate: 2.815e-05
Epoch: 162000 - Loss: 1.600e-04 - Learning Rate: 2.815e-05
Epoch: 164000 - Loss: 1.560e-04 - Learning Rate: 2.815e-05
Epoch: 166000 - Loss: 1.543e-04 - Learning Rate: 2.815e-05
Epoch: 168000 - Loss: 1.495e-04 - Learning Rate: 2.815e-05
Epoch: 170000 - Loss: 1.449e-04 - Learning Rate: 2.252e-05
Epoch: 172000 - Loss: 1.418e-04 - Learning Rate: 2.252e-05
Epoch: 174000 - Loss: 1.381e-04 - Learning Rate: 2.252e-05
Epoch: 176000 - Loss: 1.356e-04 - Learning Rate: 2.252e-05
Epoch: 178000 - Loss: 1.319e-04 - Learning Rate: 2.252e-05
Epoch: 180000 - Loss: 1.293e-04 - Learning Rate: 1.801e-05
Epoch: 182000 - Loss: 1.262e-04 - Learning Rate: 1.801e-05
Epoch: 184000 - Loss: 1.236e-04 - Learning Rate: 1.801e-05
Epoch: 186000 - Loss: 1.210e-04 - Learning Rate: 1.801e-05
Epoch: 188000 - Loss: 1.186e-04 - Learning Rate: 1.801e-05
Epoch: 190000 - Loss: 1.159e-04 - Learning Rate: 1.441e-05
Epoch: 192000 - Loss: 1.136e-04 - Learning Rate: 1.441e-05
Epoch: 194000 - Loss: 1.119e-04 - Learning Rate: 1.441e-05
Epoch: 196000 - Loss: 1.091e-04 - Learning Rate: 1.441e-05
Epoch: 198000 - Loss: 1.072e-04 - Learning Rate: 1.441e-05
Epoch: 200000 - Loss: 1.050e-04 - Learning Rate: 1.153e-05
Epoch: 202000 - Loss: 1.035e-04 - Learning Rate: 1.153e-05
Epoch: 204000 - Loss: 1.012e-04 - Learning Rate: 1.153e-05
Epoch: 206000 - Loss: 9.929e-05 - Learning Rate: 1.153e-05
Epoch: 208000 - Loss: 9.759e-05 - Learning Rate: 1.153e-05
Epoch: 210000 - Loss: 9.634e-05 - Learning Rate: 9.223e-06
Epoch: 212000 - Loss: 9.434e-05 - Learning Rate: 9.223e-06
Epoch: 214000 - Loss: 9.255e-05 - Learning Rate: 9.223e-06
Epoch: 216000 - Loss: 9.094e-05 - Learning Rate: 9.223e-06
Epoch: 218000 - Loss: 8.943e-05 - Learning Rate: 9.223e-06
Epoch: 220000 - Loss: 8.800e-05 - Learning Rate: 7.379e-06
Epoch: 222000 - Loss: 8.653e-05 - Learning Rate: 7.379e-06
Epoch: 224000 - Loss: 8.514e-05 - Learning Rate: 7.379e-06
Epoch: 226000 - Loss: 8.400e-05 - Learning Rate: 7.379e-06
Epoch: 228000 - Loss: 8.290e-05 - Learning Rate: 7.379e-06
Epoch: 230000 - Loss: 8.121e-05 - Learning Rate: 5.903e-06
Epoch: 232000 - Loss: 7.999e-05 - Learning Rate: 5.903e-06
Epoch: 234000 - Loss: 7.878e-05 - Learning Rate: 5.903e-06
Epoch: 236000 - Loss: 7.762e-05 - Learning Rate: 5.903e-06
Epoch: 238000 - Loss: 7.648e-05 - Learning Rate: 5.903e-06
Epoch: 240000 - Loss: 7.539e-05 - Learning Rate: 4.722e-06
Epoch: 242000 - Loss: 7.432e-05 - Learning Rate: 4.722e-06
Epoch: 244000 - Loss: 7.326e-05 - Learning Rate: 4.722e-06
Epoch: 246000 - Loss: 7.224e-05 - Learning Rate: 4.722e-06
Epoch: 248000 - Loss: 7.128e-05 - Learning Rate: 4.722e-06
Epoch: 250000 - Loss: 7.029e-05 - Learning Rate: 3.778e-06
Epoch: 252000 - Loss: 6.938e-05 - Learning Rate: 3.778e-06
Epoch: 254000 - Loss: 6.844e-05 - Learning Rate: 3.778e-06
Epoch: 256000 - Loss: 6.758e-05 - Learning Rate: 3.778e-06
Epoch: 258000 - Loss: 6.666e-05 - Learning Rate: 3.778e-06
Epoch: 260000 - Loss: 6.581e-05 - Learning Rate: 3.022e-06
Epoch: 262000 - Loss: 6.501e-05 - Learning Rate: 3.022e-06
Epoch: 264000 - Loss: 6.420e-05 - Learning Rate: 3.022e-06
Epoch: 266000 - Loss: 6.341e-05 - Learning Rate: 3.022e-06
Epoch: 268000 - Loss: 6.263e-05 - Learning Rate: 3.022e-06
Epoch: 270000 - Loss: 6.191e-05 - Learning Rate: 2.418e-06
Epoch: 272000 - Loss: 6.116e-05 - Learning Rate: 2.418e-06
Epoch: 274000 - Loss: 6.044e-05 - Learning Rate: 2.418e-06
/mnt/data1/users/caro_mathmode/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 276000 - Loss: 5.974e-05 - Learning Rate: 2.418e-06
Epoch: 278000 - Loss: 5.906e-05 - Learning Rate: 2.418e-06
Epoch: 280000 - Loss: 5.839e-05 - Learning Rate: 1.934e-06
Epoch: 282000 - Loss: 5.775e-05 - Learning Rate: 1.934e-06
Epoch: 284000 - Loss: 5.710e-05 - Learning Rate: 1.934e-06
Epoch: 286000 - Loss: 5.647e-05 - Learning Rate: 1.934e-06
Epoch: 288000 - Loss: 5.586e-05 - Learning Rate: 1.934e-06
Epoch: 290000 - Loss: 5.526e-05 - Learning Rate: 1.547e-06
Epoch: 292000 - Loss: 5.469e-05 - Learning Rate: 1.547e-06
Epoch: 294000 - Loss: 5.412e-05 - Learning Rate: 1.547e-06
Epoch: 296000 - Loss: 5.356e-05 - Learning Rate: 1.547e-06
Epoch: 298000 - Loss: 5.300e-05 - Learning Rate: 1.547e-06
Epoch: 300000 - Loss: 5.246e-05 - Learning Rate: 1.238e-06
Epoch: 302000 - Loss: 5.195e-05 - Learning Rate: 1.238e-06
Epoch: 304000 - Loss: 5.143e-05 - Learning Rate: 1.238e-06
Epoch: 306000 - Loss: 5.093e-05 - Learning Rate: 1.238e-06
Epoch: 308000 - Loss: 5.043e-05 - Learning Rate: 1.238e-06
Epoch: 310000 - Loss: 4.995e-05 - Learning Rate: 9.904e-07
Epoch: 312000 - Loss: 4.948e-05 - Learning Rate: 9.904e-07
Epoch: 314000 - Loss: 4.902e-05 - Learning Rate: 9.904e-07
Epoch: 316000 - Loss: 4.856e-05 - Learning Rate: 9.904e-07
Epoch: 318000 - Loss: 4.811e-05 - Learning Rate: 9.904e-07
Epoch: 320000 - Loss: 4.767e-05 - Learning Rate: 7.923e-07
Epoch: 322000 - Loss: 4.725e-05 - Learning Rate: 7.923e-07
Epoch: 324000 - Loss: 4.683e-05 - Learning Rate: 7.923e-07
Epoch: 326000 - Loss: 4.642e-05 - Learning Rate: 7.923e-07
Epoch: 328000 - Loss: 4.601e-05 - Learning Rate: 7.923e-07
Epoch: 330000 - Loss: 4.561e-05 - Learning Rate: 6.338e-07
Epoch: 332000 - Loss: 4.523e-05 - Learning Rate: 6.338e-07
Epoch: 334000 - Loss: 4.485e-05 - Learning Rate: 6.338e-07
Epoch: 336000 - Loss: 4.448e-05 - Learning Rate: 6.338e-07
Epoch: 338000 - Loss: 4.412e-05 - Learning Rate: 6.338e-07
Epoch: 340000 - Loss: 4.376e-05 - Learning Rate: 5.071e-07
Epoch: 342000 - Loss: 4.341e-05 - Learning Rate: 5.071e-07
Epoch: 344000 - Loss: 4.307e-05 - Learning Rate: 5.071e-07
Epoch: 346000 - Loss: 4.274e-05 - Learning Rate: 5.071e-07
Epoch: 348000 - Loss: 4.241e-05 - Learning Rate: 5.071e-07
Epoch: 350000 - Loss: 4.208e-05 - Learning Rate: 4.056e-07
Epoch: 352000 - Loss: 4.178e-05 - Learning Rate: 4.056e-07
Epoch: 354000 - Loss: 4.147e-05 - Learning Rate: 4.056e-07
Epoch: 356000 - Loss: 4.117e-05 - Learning Rate: 4.056e-07
Epoch: 358000 - Loss: 4.087e-05 - Learning Rate: 4.056e-07
Epoch: 360000 - Loss: 4.058e-05 - Learning Rate: 3.245e-07
Epoch: 362000 - Loss: 4.030e-05 - Learning Rate: 3.245e-07
Epoch: 364000 - Loss: 4.002e-05 - Learning Rate: 3.245e-07
Epoch: 366000 - Loss: 3.975e-05 - Learning Rate: 3.245e-07
Epoch: 368000 - Loss: 3.948e-05 - Learning Rate: 3.245e-07
Epoch: 370000 - Loss: 3.921e-05 - Learning Rate: 2.596e-07
Epoch: 372000 - Loss: 3.896e-05 - Learning Rate: 2.596e-07
Epoch: 374000 - Loss: 3.871e-05 - Learning Rate: 2.596e-07
Epoch: 376000 - Loss: 3.846e-05 - Learning Rate: 2.596e-07
Epoch: 378000 - Loss: 3.821e-05 - Learning Rate: 2.596e-07
Epoch: 380000 - Loss: 3.797e-05 - Learning Rate: 2.077e-07
Epoch: 382000 - Loss: 3.775e-05 - Learning Rate: 2.077e-07
Epoch: 384000 - Loss: 3.753e-05 - Learning Rate: 2.077e-07
Epoch: 386000 - Loss: 3.731e-05 - Learning Rate: 2.077e-07
Epoch: 388000 - Loss: 3.709e-05 - Learning Rate: 2.077e-07
Epoch: 390000 - Loss: 3.687e-05 - Learning Rate: 1.662e-07
Epoch: 392000 - Loss: 3.667e-05 - Learning Rate: 1.662e-07
Epoch: 394000 - Loss: 3.647e-05 - Learning Rate: 1.662e-07
Epoch: 396000 - Loss: 3.627e-05 - Learning Rate: 1.662e-07
Epoch: 398000 - Loss: 3.607e-05 - Learning Rate: 1.662e-07
Epoch: 400000 - Loss: 3.588e-05 - Learning Rate: 1.329e-07
Epoch: 402000 - Loss: 3.570e-05 - Learning Rate: 1.329e-07
Epoch: 404000 - Loss: 3.552e-05 - Learning Rate: 1.329e-07
Epoch: 406000 - Loss: 3.534e-05 - Learning Rate: 1.329e-07
Epoch: 408000 - Loss: 3.516e-05 - Learning Rate: 1.329e-07
Epoch: 410000 - Loss: 3.499e-05 - Learning Rate: 1.063e-07
Epoch: 412000 - Loss: 3.483e-05 - Learning Rate: 1.063e-07
Epoch: 414000 - Loss: 3.467e-05 - Learning Rate: 1.063e-07
Epoch: 416000 - Loss: 3.451e-05 - Learning Rate: 1.063e-07
Epoch: 418000 - Loss: 3.435e-05 - Learning Rate: 1.063e-07
Epoch: 420000 - Loss: 3.419e-05 - Learning Rate: 8.507e-08
Epoch: 422000 - Loss: 3.405e-05 - Learning Rate: 8.507e-08
Epoch: 424000 - Loss: 3.391e-05 - Learning Rate: 8.507e-08
Epoch: 426000 - Loss: 3.377e-05 - Learning Rate: 8.507e-08
Epoch: 428000 - Loss: 3.363e-05 - Learning Rate: 8.507e-08
Epoch: 430000 - Loss: 3.349e-05 - Learning Rate: 6.806e-08
Epoch: 432000 - Loss: 3.337e-05 - Learning Rate: 6.806e-08
Epoch: 434000 - Loss: 3.324e-05 - Learning Rate: 6.806e-08
Epoch: 436000 - Loss: 3.312e-05 - Learning Rate: 6.806e-08
Epoch: 438000 - Loss: 3.300e-05 - Learning Rate: 6.806e-08
Epoch: 440000 - Loss: 3.287e-05 - Learning Rate: 5.445e-08
Epoch: 442000 - Loss: 3.277e-05 - Learning Rate: 5.445e-08
Epoch: 444000 - Loss: 3.266e-05 - Learning Rate: 5.445e-08
Epoch: 446000 - Loss: 3.255e-05 - Learning Rate: 5.445e-08
Epoch: 448000 - Loss: 3.244e-05 - Learning Rate: 5.445e-08
Epoch: 450000 - Loss: 3.233e-05 - Learning Rate: 4.356e-08
Epoch: 452000 - Loss: 3.224e-05 - Learning Rate: 4.356e-08
Epoch: 454000 - Loss: 3.214e-05 - Learning Rate: 4.356e-08
Epoch: 456000 - Loss: 3.204e-05 - Learning Rate: 4.356e-08
Epoch: 458000 - Loss: 3.195e-05 - Learning Rate: 4.356e-08
Epoch: 460000 - Loss: 3.185e-05 - Learning Rate: 3.484e-08
Epoch: 462000 - Loss: 3.178e-05 - Learning Rate: 3.484e-08
Epoch: 464000 - Loss: 3.171e-05 - Learning Rate: 3.484e-08
Epoch: 466000 - Loss: 3.164e-05 - Learning Rate: 3.484e-08
Epoch: 468000 - Loss: 3.157e-05 - Learning Rate: 3.484e-08
Epoch: 470000 - Loss: 3.150e-05 - Learning Rate: 2.788e-08
Epoch: 472000 - Loss: 3.145e-05 - Learning Rate: 2.788e-08
Epoch: 474000 - Loss: 3.139e-05 - Learning Rate: 2.788e-08
Epoch: 476000 - Loss: 3.134e-05 - Learning Rate: 2.788e-08
Epoch: 478000 - Loss: 3.128e-05 - Learning Rate: 2.788e-08
Epoch: 480000 - Loss: 3.123e-05 - Learning Rate: 2.230e-08
Epoch: 482000 - Loss: 3.118e-05 - Learning Rate: 2.230e-08
Epoch: 484000 - Loss: 3.113e-05 - Learning Rate: 2.230e-08
Epoch: 486000 - Loss: 3.109e-05 - Learning Rate: 2.230e-08
Epoch: 488000 - Loss: 3.104e-05 - Learning Rate: 2.230e-08
Epoch: 490000 - Loss: 3.100e-05 - Learning Rate: 1.784e-08
Epoch: 492000 - Loss: 3.096e-05 - Learning Rate: 1.784e-08
Epoch: 494000 - Loss: 3.092e-05 - Learning Rate: 1.784e-08
Epoch: 496000 - Loss: 3.088e-05 - Learning Rate: 1.784e-08
Epoch: 498000 - Loss: 3.084e-05 - Learning Rate: 1.784e-08
computing time 492.47307330767313 [min]
