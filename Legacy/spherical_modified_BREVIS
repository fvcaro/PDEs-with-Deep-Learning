#!/usr/bin/env python
# coding: utf-8

# In[1]:


import torch
import torch.nn as nn
from torch.optim.lr_scheduler import StepLR
from matplotlib import pyplot as plt
import numpy as np
import os
from time import time


# In[2]:


print('torch version:',torch.__version__)

if torch.cuda.is_available():
    device = torch.device("cuda")
    print('device:', device)
else:
    device = torch.device("cpu")
    print('CUDA is not available. Using CPU.')


# In[3]:


# PDE parameters
L  = 40
T  = 30
x0 = 15
A  = 1.
sigma = 1.
beta  = 0.
gamma = -5.e-6


# In[4]:


# Define directory to save results
save_dir = 'results_hades_kessence'
os.makedirs(save_dir, exist_ok=True)


# In[5]:


# Training parameters
GAMMA1 = 10.
GAMMA2 = 1.
GAMMA3 = 10.
#
TRAIN_DOM_POINTS = 262144 #131072
TRAIN_BC_POINTS  = 2048   #1024
TRAIN_IC_POINTS  = 2048
#
EPOCHS        = 400000
LEARNING_RATE = 0.001


# In[6]:


# NN class
class Model(nn.Module):
    def __init__(self, layer_sizes, activation=nn.Tanh(),seed=42):
        super(Model, self).__init__()
        self.layers = nn.ModuleList()
        self.activation = activation
        self.seed = seed
        # Fix seed for reproducibility
        torch.manual_seed(seed)
        #
        for i in range(len(layer_sizes) - 1):
            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            # Adding activation function for all but the last layer
            if i < len(layer_sizes) - 2:
                self.layers.append(self.activation)  
        # Initialize weights using Glorot initialization
        self.init_weights()  
    #
    def init_weights(self):
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                # Glorot initialization
                # nn.init.xavier_uniform_(layer.weight)  
                # Replace Glorot initialization with Kaiming initialization
                nn.init.kaiming_uniform_(layer.weight, nonlinearity='tanh')
                # Initialize bias to zeros
                nn.init.constant_(layer.bias, 0.0)  
    #
    def forward(self, x, y):
        #
        inputs = torch.cat([x, y], axis=1)
        #
        for layer in self.layers:
            inputs = layer(inputs)
        return inputs


# In[7]:


def lossRes(r,t):
    u = model(r,t)
    # Derivatives
    u_t  = torch.autograd.grad(outputs=u, inputs=t,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u)
                              )[0]
    u_tt = torch.autograd.grad(outputs=u_t, inputs=t,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_t)
                              )[0]
    u_tr = torch.autograd.grad(outputs=u_t, inputs=r,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_t)
                              )[0]
    u_r  = torch.autograd.grad(outputs=u, inputs=r,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u)
                              )[0]
    u_rr = torch.autograd.grad(outputs=u_r, inputs=r,
                               create_graph=True,
                               grad_outputs=torch.ones_like(u_r)
                               )[0]
    #
    X    = u_r**2 - u_t**2
    K_X  = -sigma/2 + beta*X/2 + 3*gamma*(X**2)/8
    K_XX = beta/2 + 3*gamma*X/4
    #
    res1 = 2*r*K_XX*(u_rr)*(u_r**2)/K_X
    res2 = 4*r*K_XX*(u_t)*(u_tr)*(u_r)/K_X
    res3 = 2*r*K_XX*(u_t**2)*(u_tt)/K_X
    res4 = 2*u_r + r*u_rr - r*u_tt 
    residual = res1 - res2 + res3 + res4 
    loss_dom = residual 
    return loss_dom


# In[8]:


def lossBCleft(r_bc,t_bc):
    u_bc    = model(r_bc,t_bc)
    # Derivatives
    u_bc_r  = torch.autograd.grad(outputs=u_bc, inputs=r_bc,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_bc)
                              )[0]
    #
    loss_bc = torch.pow(u_bc_r - 0.,2)
    return loss_bc

def lossBCright(r_bc,t_bc):
    u_bc    = model(r_bc,t_bc)
    # Derivatives
    u_bc_r  = torch.autograd.grad(outputs=u_bc, inputs=r_bc,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_bc)
                              )[0]
    u_bc_t  = torch.autograd.grad(outputs=u_bc, inputs=t_bc,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_bc)
                              )[0]
    #
    loss_bc = torch.pow(u_bc_t + u_bc_r + u_bc/r_bc -  0.,2)
    return loss_bc
#
def lossIC(r_ic,t_ic):
    u_ic     = model(r_ic,t_ic)
    # Derivatives
    u_ic_t   = torch.autograd.grad(outputs=u_ic, 
                              inputs=t_ic,
                              create_graph=True,
                              grad_outputs=torch.ones_like(u_ic)
                              )[0]
    #
    loss_ic  = torch.pow(u_ic - A*torch.exp(-torch.pow((r_ic - x0),2)),2)
    loss_ic += torch.pow(u_ic_t - 0.,2)
    return loss_ic


# In[9]:


def random_domain_points(R, T, n=8192):
    r = R*torch.rand(n, 1, device=device, requires_grad=True)
    t = T*torch.rand(n, 1, device=device, requires_grad=True)
    return r, t
def random_BC_points_L(R, T, n=512):
    r = 0*torch.ones((n, 1), dtype=torch.float32, device=device, requires_grad=True)
    t = T*torch.rand(n, 1, device=device, requires_grad=True)
    return r, t
def random_BC_points_R(R, T, n=512):
    r = R*torch.ones((n, 1), dtype=torch.float32, device=device, requires_grad=True)
    t = T*torch.rand(n, 1, device=device, requires_grad=True)
    return r, t
def random_IC_points(R, n=128):
    r = R*torch.rand(n, 1, device=device, requires_grad=True)
    t = torch.zeros(n, 1, device=device, requires_grad=True)
    return r, t


# In[10]:


torch.manual_seed(42)
model = Model([2,64,64,64,64,1]).to(device)

# load model
filename = os.path.join(save_dir, f'initial_trained_model')
model.load_state_dict(torch.load(filename,map_location=torch.device(device)))
model.eval()

r,      t      = random_domain_points(L,T,n=TRAIN_DOM_POINTS)
r_bc  , t_bc   = random_BC_points_L(L,T,n=TRAIN_BC_POINTS)
r_bc_R, t_bc_R = random_BC_points_R(L,T,n=TRAIN_BC_POINTS)
r_ic,   t_ic   = random_IC_points(L,n=TRAIN_IC_POINTS)
#
fig = plt.figure(figsize=(7,6))
# Fontsize of evething inside the plot
plt.rcParams.update({'font.size': 16})
#
plt.plot(r.cpu().detach().numpy(),t.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_bc.cpu().detach().numpy(),t_bc.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_bc_R.cpu().detach().numpy(),t_bc_R.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_ic.cpu().detach().numpy(),t_ic.cpu().detach().numpy(),'o',ms=1)
#
filename = os.path.join(save_dir, f'mesh_initial.png')
plt.savefig(filename, dpi=300, facecolor=None, edgecolor=None,
            orientation='portrait', format='png',transparent=True, 
            bbox_inches='tight', pad_inches=0.1, metadata=None)
plt.close()


# In[12]:


filename = os.path.join(save_dir, f'initial_sampling_points')
np.savez(filename,
         r=r.cpu().detach().numpy(),
         t=t.cpu().detach().numpy(),
         r_bc_R=r_bc_R.cpu().detach().numpy(),
         t_bc_R=t_bc_R.cpu().detach().numpy(),
         r_bc=r_bc.cpu().detach().numpy(),
         t_bc=t_bc.cpu().detach().numpy(),
         r_ic=r_ic.cpu().detach().numpy(),
         t_ic=t_ic.cpu().detach().numpy()
        )


# In[15]:


loss_dom_list  = []
loss_bc_L_list = []
loss_bc_R_list = []
loss_ic_list   = []
#
t0 = time()
# Initial training set
stop_criteria = 100.
best_achieved = 100.
adapt_step    = 0

#
optimizer = torch.optim.Adam(model.parameters(),
                            lr=LEARNING_RATE)
scheduler = StepLR(optimizer, step_size=10000, gamma=.8, verbose=False)  # Learning rate scheduler
#
print(f'Adaptive step: {adapt_step}')
for epoch in range(EPOCHS):
    # To make the gradients zero
    optimizer.zero_grad() 
    # RESIDUAL 
    residual   = lossRes(r,t)
    #residual_r = torch.autograd.grad(outputs=residual, 
    #                                 inputs=r, 
    #                                 create_graph=True, 
    #                                 grad_outputs=torch.ones_like(residual)
    #                                )[0]
    loss_dom  = torch.mean(torch.pow(residual,2)) #+ torch.mean(torch.pow(residual_r,2))
    # BCs
    loss_bc_L = torch.mean(lossBCleft(r_bc,t_bc))
    loss_bc_R = torch.mean(lossBCright(r_bc_R,t_bc_R))
    # IC 
    loss_ic  = torch.mean(lossIC(r_ic,t_ic))
    # LOSS
    loss = loss_dom + GAMMA1*loss_bc_L + GAMMA2*loss_bc_R + GAMMA3*loss_ic
    #
    loss_dom_list.append(loss_dom.item())
    loss_bc_L_list.append(loss_bc_L.item())
    loss_bc_R_list.append(loss_bc_R.item())
    loss_ic_list.append(loss_ic.item())
    #
    loss.backward(retain_graph=True) 
    optimizer.step() # 
    scheduler.step()  # Update learning rate
    if epoch % 2000 == 0:
        print(f"Epoch: {epoch} - Loss: {loss.item():>1.3e} - Learning Rate: {scheduler.get_last_lr()[0]:>1.3e}")
    #
    #
print('computing time',(time() - t0)/60,'[min]')  


# In[17]:


filename = os.path.join(save_dir, f'final_trained_model')
torch.save(model.state_dict(), filename)
#
filename = os.path.join(save_dir, f'training_losses')
np.savez(filename,
         loss_dom_list=loss_dom_list,
         loss_bc_R_list=loss_bc_R_list,
         loss_bc_L_list=loss_bc_L_list,
         loss_ic_list=loss_ic_list
        )


# In[18]:


fig = plt.figure(figsize=(7,6))
# Fontsize of evething inside the plot
plt.rcParams.update({'font.size': 16})
#
plt.plot(r.cpu().detach().numpy(),t.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_bc.cpu().detach().numpy(),t_bc.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_bc_R.cpu().detach().numpy(),t_bc_R.cpu().detach().numpy(),'o',ms=1)
plt.plot(r_ic.cpu().detach().numpy(),t_ic.cpu().detach().numpy(),'o',ms=1)
#
filename = os.path.join(save_dir, f'mesh_final.png')
plt.savefig(filename, dpi=300, facecolor=None, edgecolor=None,
            orientation='portrait', format='png',transparent=True, 
            bbox_inches='tight', pad_inches=0.1, metadata=None)
plt.close()


# In[19]:


fig = plt.figure(figsize=(7,6))
# Fontsize of evething inside the plot
plt.rcParams.update({'font.size': 16})
#
plt.semilogy(loss_ic_list, label='IC Loss')
plt.semilogy(loss_bc_R_list, label='Right BC Loss')
plt.semilogy(loss_bc_L_list, label='Left BC Loss')
plt.semilogy(loss_dom_list, label='Domain Loss')
plt.grid(True, which="both", ls="--")
plt.legend()
#
filename = os.path.join(save_dir, f'training_losses.png')
plt.savefig(filename, dpi=300, facecolor=None, edgecolor=None,
            orientation='portrait', format='png',transparent=True, 
            bbox_inches='tight', pad_inches=0.1, metadata=None)
plt.close()


# In[20]:


x = torch.linspace(0,L,1024).view(-1,1)
x =x.to(device)
for t_i in np.linspace(0,T,2*T+1):
    t = t_i*torch.ones_like(x)
    t = t.to(device)
    nn_sol = model(x,t).cpu().detach().numpy()
    #
    plt.figure()
    plt.plot(x.cpu(),nn_sol,linewidth='2')
    plt.title(r'$t_i$:'+str(t_i))
    plt.xlim(0,L)
    #
    filename = os.path.join(save_dir, f'pinns_sol_{t_i}.png')
    plt.savefig(filename, dpi=300, facecolor=None, edgecolor=None,
            orientation='portrait', format='png',transparent=True, 
            bbox_inches='tight', pad_inches=0.1, metadata=None)
    plt.close()


# In[ ]: